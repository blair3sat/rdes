{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, merge\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAutoencoder():\n",
    "    '''\n",
    "    Implementation of an Adversarial Autoencoder (a type of GAN) modified\n",
    "    from https://github.com/eriklindernoren/Keras-GAN/blob/master/aae/aae.py\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.img_rows = 72\n",
    "        self.img_cols = 2\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        self.latent_dim = 10\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the encoder / decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        # The generator takes the image, encodes it and reconstructs it\n",
    "        # from the encoding\n",
    "        encoded_repr = self.encoder(img)\n",
    "        reconstructed_img = self.decoder(encoded_repr)\n",
    "\n",
    "        # For the adversarial_autoencoder model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator determines validity of the encoding\n",
    "        validity = self.discriminator(encoded_repr)\n",
    "\n",
    "        # The adversarial_autoencoder model  (stacked generator and discriminator)\n",
    "        self.adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
    "        self.adversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'],\n",
    "            loss_weights=[0.999, 0.001],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        # Encoder\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        h = Flatten()(img)\n",
    "        h = Dense(512)(h)\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dense(512)(h)\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        mu = Dense(self.latent_dim)(h)\n",
    "        log_var = Dense(self.latent_dim)(h)\n",
    "        latent_repr = merge([mu, log_var],\n",
    "                mode=lambda p: p[0] + K.random_normal(K.shape(p[0])) * K.exp(p[1] / 2),\n",
    "                output_shape=lambda p: p[0])\n",
    "\n",
    "        return Model(img, latent_repr)\n",
    "\n",
    "    def build_decoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = model(z)\n",
    "\n",
    "        return Model(z, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.summary()\n",
    "\n",
    "        encoded_repr = Input(shape=(self.latent_dim, ))\n",
    "        validity = model(encoded_repr)\n",
    "\n",
    "        return Model(encoded_repr, validity)\n",
    "\n",
    "    def train(self, epochs, data, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            latent_fake = self.encoder.predict(imgs)\n",
    "            latent_real = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(latent_real, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "#             if epoch % sample_interval == 0:\n",
    "#                 self.sample_images(epoch)\n",
    "\n",
    "#     def sample_images(self, epoch):\n",
    "#         r, c = 5, 5\n",
    "\n",
    "#         z = np.random.normal(size=(r*c, self.latent_dim))\n",
    "#         gen_imgs = self.decoder.predict(z)\n",
    "\n",
    "#         gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "#         cnt = 0\n",
    "#         for i in range(r):\n",
    "#             for j in range(c):\n",
    "#                 axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "#                 axs[i,j].axis('off')\n",
    "#                 cnt += 1\n",
    "#         fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "#         plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"aae_generator\")\n",
    "        save(self.discriminator, \"aae_discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[9.0000e+01 9.5000e+01 9.8764e+01 ... 4.1000e+02 4.1500e+02 4.2000e+02]\n",
      "  [4.9500e+02 4.6000e+04 5.6800e+04 ... 5.1700e+04 4.8500e+04 4.5500e+04]]\n",
      "\n",
      " [[9.0000e+01 9.5000e+01 9.8220e+01 ... 4.1000e+02 4.1500e+02 4.2000e+02]\n",
      "  [4.9500e+02 7.9700e+04 9.5100e+04 ... 2.4200e+04 2.2600e+04 2.1100e+04]]\n",
      "\n",
      " [[9.0142e+01 9.5000e+01 1.0000e+02 ... 4.1500e+02 4.2000e+02 4.2500e+02]\n",
      "  [4.9500e+02 7.2200e+03 1.2200e+04 ... 2.0200e+04 1.8700e+04 1.7300e+04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[9.0000e+01 9.5000e+01 1.0000e+02 ... 4.0500e+02 4.1000e+02 4.1500e+02]\n",
      "  [4.9500e+02 5.9300e+04 8.6600e+04 ... 5.7500e+04 5.4300e+04 5.1300e+04]]\n",
      "\n",
      " [[9.0000e+01 9.5000e+01 9.9301e+01 ... 4.1000e+02 4.1500e+02 4.2000e+02]\n",
      "  [4.9500e+02 3.1100e+04 3.9800e+04 ... 2.4600e+04 2.3100e+04 2.1800e+04]]\n",
      "\n",
      " [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "  [0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "directory = os.fsencode('data/processed/')\n",
    "\n",
    "tdata = np.zeros(shape=(len(os.listdir(directory)), 2, 72))\n",
    "\n",
    "k=0\n",
    "for file in os.listdir(directory):\n",
    "    fn = os.fsdecode(file)\n",
    "    if fn.endswith('.csv'): \n",
    "        df = pd.read_csv('data/processed/' + fn)\n",
    "        \n",
    "        ic = np.array(df['Height'][0:72])\n",
    "        oc = np.array(df['Density'][0:72])\n",
    "        tdata[k] = np.array([ic, oc])\n",
    "        k += 1\n",
    "        \n",
    "\n",
    "print(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 137,217\n",
      "Trainable params: 137,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-554ceb8ad23c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdversarialAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-91de4a19b6af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Build the encoder / decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-91de4a19b6af>\u001b[0m in \u001b[0;36mbuild_encoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         latent_repr = merge([mu, log_var],\n\u001b[1;32m     58\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 output_shape=lambda p: p[0])\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "aae = AdversarialAutoencoder()\n",
    "aae.train(tdata, epochs=20000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfls = []\n",
    "# mn = ''\n",
    "# directory = os.fsencode('data/processed/')\n",
    "# for file in os.listdir(directory):\n",
    "#     fn = os.fsdecode(file)\n",
    "#     if fn.endswith('.csv'): \n",
    "#         df = pd.read_csv('data/processed/' + fn)\n",
    "        \n",
    "#         if len(dfls) > 0 and len(df) < min(dfls):\n",
    "#             mn = fn\n",
    "#         dfls.append(len(df))\n",
    "        \n",
    "#         print(fn)\n",
    "#         plt.plot(df['Density'][0:72], df['Height'][0:72])\n",
    "#         plt.show()\n",
    "#         print('\\n\\n')\n",
    "\n",
    "# print(min(dfls))\n",
    "# print(mn)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
